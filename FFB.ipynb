{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import pinv\n",
    "from scipy.stats import bernoulli\n",
    "from math import log\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinTS:\n",
    "    def __init__(self, d, sigma, lambda_reg=1.0, alpha=1.0):\n",
    "        \"\"\"\n",
    "        LinTS algorithm for linear contextual bandits.\n",
    "\n",
    "        Parameters:\n",
    "        - d: Dimension of the action space.\n",
    "        - sigma: Standard deviation of the noise (assumes sub-Gaussian noise).\n",
    "        - lambda_reg: Regularization parameter for the covariance matrix.\n",
    "        - alpha: Scaling factor for exploration (optional, typically set to 1.0).\n",
    "        \"\"\"\n",
    "        self.d = d  # Dimension of the feature space\n",
    "        self.sigma = sigma  # Noise standard deviation\n",
    "        self.lambda_reg = lambda_reg  # Regularization parameter\n",
    "        self.alpha = alpha  # Exploration scaling factor\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all internal estimators and counters for independent experiments.\n",
    "        \"\"\"\n",
    "        self.t = 0  # Time step counter\n",
    "        self.hat_theta = np.zeros(self.d)  # Estimated parameter vector\n",
    "        self.cov = 1/self.lambda_reg * np.identity(self.d)  # Covariance matrix (B_t)\n",
    "        self.invcov = pinv(self.cov)  # Inverse of the covariance matrix\n",
    "        self.b_t = np.zeros(self.d)  # Accumulated rewards vector\n",
    "\n",
    "    def get_action(self, arms):\n",
    "        \"\"\"\n",
    "        Choose an action using the LinTS strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - arms: Array of shape (K, d) representing the available actions.\n",
    "\n",
    "        Returns:\n",
    "        - The chosen action (vector of dimension d).\n",
    "        \"\"\"\n",
    "        # Sample a parameter vector from the posterior distribution\n",
    "        sampled_theta = np.random.multivariate_normal(self.hat_theta, self.invcov)\n",
    "        estimated_rewards = np.dot(arms, sampled_theta)\n",
    "        best_arm_idx = np.argmax(estimated_rewards) # Choose the action with the highest estimated reward\n",
    "        return arms[best_arm_idx]\n",
    "\n",
    "    def receive_reward(self, chosen_arm, reward):\n",
    "        \"\"\"\n",
    "        Update the internal model with the observed reward for the chosen action.\n",
    "\n",
    "        Parameters:\n",
    "        - chosen_arm: The action (vector) that was chosen.\n",
    "        - reward: The observed reward.\n",
    "        \"\"\"\n",
    "        # Update covariance matrix and its inverse\n",
    "        self.cov += 1/self.sigma**2*np.outer(chosen_arm, chosen_arm)\n",
    "        self.invcov = pinv(self.cov)\n",
    "\n",
    "        # Update the b_t vector with the reward\n",
    "        self.b_t += 1/self.sigma**2*(reward * chosen_arm)\n",
    "\n",
    "        # Update the least squares estimate of theta\n",
    "        self.hat_theta = np.dot(self.invcov, self.b_t)\n",
    "        self.t += 1\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns the name of the algorithm for identification.\n",
    "        \"\"\"\n",
    "        return f'LinTS(alpha={self.alpha})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment, agent, Nmc, T, pseudo_regret=True):\n",
    "    \"\"\"\n",
    "    Play one Nmc trajectories over a horizon T for the specified agent.\n",
    "    Return the agent's name (sring) and the collected data in an nd-array.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.zeros((Nmc, T))\n",
    "\n",
    "\n",
    "\n",
    "    for n in range(Nmc):\n",
    "        agent.reset()\n",
    "        for t in range(T):\n",
    "            action_set = environment.get_action_set()\n",
    "            action = agent.get_action(action_set)\n",
    "            reward = environment.get_reward(action)\n",
    "            agent.receive_reward(action,reward)\n",
    "\n",
    "            # compute instant (pseudo) regret\n",
    "            means = environment.get_means()\n",
    "            best_reward = np.max(means)\n",
    "            if pseudo_regret:\n",
    "              # pseudo-regret removes some of the noise and corresponds to the metric studied in class\n",
    "              data[n,t] = best_reward - np.dot(environment.theta,action)\n",
    "            else:\n",
    "              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
    "\n",
    "    return agent.name(), data\n",
    "\n",
    "\n",
    "def experiment(environment, agents, Nmc, T,pseudo_regret=True):\n",
    "    \"\"\"\n",
    "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for agent in agents:\n",
    "        agent_id, regrets = play(environment, agent,Nmc, T,pseudo_regret)\n",
    "\n",
    "        all_data[agent_id] = regrets\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActionsGenerator(K, d, mean=None):\n",
    "    \"\"\"\n",
    "    Generate K actions in a d-dimensional space uniformly sampled on the unit sphere.\n",
    "\n",
    "    Parameters:\n",
    "    - K (int): Number of actions to generate.\n",
    "    - d (int): Dimension of the action space.\n",
    "    - mean (array-like, optional): Mean vector for generating actions. Defaults to the origin.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: An array of shape (K, d) with K action vectors uniformly distributed on the unit sphere.\n",
    "    \"\"\"\n",
    "    # Generate K random points from a normal distribution\n",
    "    random_points = np.random.normal(size=(K, d))\n",
    "    \n",
    "    # Normalize each vector to lie on the unit sphere\n",
    "    norms = np.linalg.norm(random_points, axis=1, keepdims=True)\n",
    "    unit_sphere_points = random_points / norms\n",
    "\n",
    "    # Optionally adjust the mean of the distribution\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean).reshape(1, -1)  # Ensure mean is compatible with unit_sphere_points\n",
    "        unit_sphere_points += mean\n",
    "        # Re-normalize to ensure points still lie on the sphere\n",
    "        unit_sphere_points = unit_sphere_points / np.linalg.norm(unit_sphere_points, axis=1, keepdims=True)\n",
    "\n",
    "    return unit_sphere_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBandit:\n",
    "\n",
    "    def __init__(self, theta, K, var=1., fixed_actions=None):\n",
    "      \"\"\"\n",
    "      theta: d-dimensional vector (bounded) representing the hidden parameter\n",
    "      K: number of actions per round (random action vectors generated each time)\n",
    "      pb_type: string in 'fixed', 'iid', 'nsr' (please ignore NotSoRandom)\n",
    "      \"\"\"\n",
    "      self.d = np.size(theta)\n",
    "      self.theta = theta\n",
    "      self.K = K\n",
    "      self.var = var\n",
    "      self.current_action_set = np.zeros(self.d)\n",
    "      if fixed_actions is not None:\n",
    "         self.pb_type = 'fixed'\n",
    "         self.fixed_actions = fixed_actions\n",
    "      else :\n",
    "         self.pb_type = 'iid'\n",
    "\n",
    "\n",
    "    def get_action_set(self):\n",
    "      \"\"\"\n",
    "      Generates a set of vectors in dimension self.d. Use your ActionsGenerator\n",
    "      Alternatively, the set of actions is fixed a priori (given as input).\n",
    "      Implement a condition to return the fixed set when one is given\n",
    "      \"\"\"\n",
    "      if self.pb_type == 'fixed':\n",
    "          # If fixed actions are provided, use them\n",
    "          self.current_action_set = self.fixed_actions\n",
    "      else:\n",
    "          # Generate K actions uniformly on the unit sphere\n",
    "          self.current_action_set = ActionsGenerator(self.K, self.d)\n",
    "      return self.current_action_set\n",
    "      #return np.eye(self.d) ## dummy return, it only returns d actions (TODO)\n",
    "\n",
    "\n",
    "    def get_reward(self, action):\n",
    "      \"\"\" sample reward given action and the model of this bandit environment\n",
    "      action: d-dimensional vector (action chosen by the learner)\n",
    "      \"\"\"\n",
    "      mean = np.dot(action, self.theta)\n",
    "      return np.random.normal(mean, scale=self.var)\n",
    "\n",
    "    def get_means(self):\n",
    "      return np.dot(self.current_action_set, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "theta = np.\n",
    "d = 1\n",
    "T = 100  # Finite Horizon\n",
    "N = 100  # Monte Carlo simulations\n",
    "K = 10\n",
    "iid_env = LinearBandit(theta, K, var=sigma**2)\n",
    "linTS = LinTS(d, sigma, lambda_reg=1.0, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linucb_vs_greedy = experiment(iid_env, [linTS], Nmc=N, T=T, pseudo_regret=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(n_arms, n_trials, true_success_probs, strategy=\"rpm\"):\n",
    "    \"\"\"Simulates an experiment with RPM ou Equal Allocation stragies\n",
    "    \n",
    "    Args :\n",
    "        n_arms (int) : number of arms/machines\n",
    "        n_trials (int) : number of trials\n",
    "        true_success_probs (array) : array containing the true success probability of each arm\n",
    "        strategy (str) : strategy used (rpm or equal allocation)\n",
    "\n",
    "    Returns : \n",
    "        regrets (list) : list of regrets at each step \n",
    "        cumulative_regret (float) : total regret at the end of the experiment\n",
    "    \"\"\"\n",
    "\n",
    "    optimal_reward = max(true_success_probs) #proba of success of the optimal arm\n",
    "\n",
    "    Y_t = np.zeros(n_arms)  # Succès cumulatifs observés pour chaque bras\n",
    "    N_t = np.zeros(n_arms)  # Nombre total cumulatifs de tirages pour chaque bras\n",
    "    \n",
    "    # n_prior_obs = 10**6\n",
    "    # champion_index = 0\n",
    "    # N_t[champion_index] = n_prior_obs\n",
    "    # prior_obs = np.random.rand(n_prior_obs) < true_success_probs[champion_index]\n",
    "    # Y_t[champion_index] = np.sum(prior_obs)\n",
    "\n",
    "    regrets = []\n",
    "    cumulative_regret = 0\n",
    "    \n",
    "    for t in range(1, n_trials + 1):\n",
    "        n_observations = 1000 # Nb of observation for one trial\n",
    "\n",
    "        # Calculer les probabilités d'optimalité\n",
    "        \n",
    "        win_probs = compute_win_prob(Y_t, N_t, 100) # w_at for each arm\n",
    "        if strategy==\"rpm\":\n",
    "            chosen_arm = np.random.choice(range(1,n_arms+1), p=win_probs, size=n_observations)\n",
    "        if strategy==\"equal allocation\":\n",
    "            chosen_arm = np.random.choice(range(1,n_arms+1), p=[1/n_arms]*n_arms, size=n_observations)\n",
    "        \n",
    "        hat_theta = np.dot(invcov, b_t)\n",
    "        cov += 1/sigma**2*np.outer(chosen_arm, chosen_arm)\n",
    "        invcov = pinv(self.cov)\n",
    "\n",
    "        # Actualiser N_t, remplir n_t\n",
    "        np.add.at(N_t, chosen_arm-1, 1) # Nombre total de tirages pour chaque bras \n",
    "        n_t = np.zeros(n_arms)  # Nombre de tirages pour chaque bras à cette étape\n",
    "        np.add.at(n_t, chosen_arm-1, 1)\n",
    "\n",
    "        # Actualiser Y_t et calculer le regret \n",
    "        regret_t = 0\n",
    "        for a in range(n_arms):\n",
    "            obs = np.random.rand(int(n_t[a])) < true_success_probs[a] # Simule des observations selon la probabilité réelle de succès de ce bras\n",
    "            Y_t[a] += np.sum(obs) #mets à jour le nombre de succès de ce bras\n",
    "\n",
    "            regret_t +=  n_t[a]*(optimal_reward - true_success_probs[a])\n",
    "        regrets.append(regret_t)\n",
    "        cumulative_regret += regret_t\n",
    "        \n",
    "        if max(win_probs)>0.95: \n",
    "            break\n",
    "\n",
    "    \n",
    "    return regrets, cumulative_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def generate_factor_vectors_with_zeros(levels):\n",
    "    factor_vectors = []\n",
    "    for level in levels:\n",
    "        # Créer des vecteurs binaires pour un facteur avec une seule position \"active\"\n",
    "        vectors = np.eye(level-1, dtype=int)\n",
    "        \n",
    "        # Ajouter un vecteur de zéros au début\n",
    "        zeros_vector = np.zeros((1, level-1), dtype=int)\n",
    "        vectors = np.vstack((zeros_vector, vectors))\n",
    "        \n",
    "        factor_vectors.append(vectors)\n",
    "    return factor_vectors\n",
    "\n",
    "def X_possibilities(levels):\n",
    "    # Produire toutes les combinaisons dans le même ordre\n",
    "    all_combinations = list(itertools.product(*generate_factor_vectors_with_zeros(levels)))\n",
    "\n",
    "    # Convertir les combinaisons en vecteurs concaténés\n",
    "    combined_vectors = [np.hstack(combination) for combination in all_combinations]\n",
    "    #ajouter l'intercept\n",
    "    combined_vectors_with_leading_one = [np.insert(vector, 0, 1) for vector in combined_vectors]\n",
    "\n",
    "    return combined_vectors_with_leading_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_possibilities([2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.dot(X_possibilities([2,3,4,5]), np.random.multivariate_normal(np.zeros(11), np.eye(11, dtype=int)).reshape(1, -1).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97881959, -1.59020863, -0.56380887, -1.77995423, -1.18016751,\n",
       "         0.58506566, -0.42694098,  0.15669665, -0.04456461, -0.3781006 ,\n",
       "         0.60540568]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multivariate_normal(np.zeros(11), np.eye(11, dtype=int)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "def generates_theta(X_t, y_t, sigma):\n",
    "    inv_omega = 1/(sigma**2)*np.dot(X_t.T, X_t) + np.eyes(np.shape(X_t)[1], dtype=int)\n",
    "    omega = pinv(inv_omega)\n",
    "\n",
    "    theta_tilde = 1/(sigma**2)*omega*(np.dot(X_t.T, y_t))\n",
    "\n",
    "    return np.random.multivariate_normal(theta_tilde, omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_reward(X_possibilities, theta):\n",
    "    values = np.dot(X_possibilities, theta.reshape(1, -1).T).reshape(1,-1)\n",
    "    \n",
    "    return values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_reward(X_possibilities, theta):\n",
    "    best_reward = np.max(calculate_reward(X_possibilities, theta))\n",
    "    \n",
    "    return best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_Ia(X_possibilities, theta):\n",
    "    # Trouver la valeur maximale\n",
    "    values = np.dot(X_possibilities, theta.reshape(1, -1).T).reshape(1,-1)\n",
    "    max_value = np.max(values)\n",
    "    \n",
    "    # Comparer chaque élément à la valeur maximale\n",
    "    Ia = (values == max_value).astype(int)\n",
    "    \n",
    "    return Ia.flatten()\n",
    "calculate_Ia(X_possibilities([2,3,4,5]), np.random.multivariate_normal(np.zeros(11), np.eye(11, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.003, 0.   , 0.004, 0.005, 0.001, 0.014, 0.007, 0.009,\n",
       "       0.003, 0.002, 0.008, 0.012, 0.009, 0.013, 0.002, 0.007, 0.011,\n",
       "       0.01 , 0.007, 0.   , 0.005, 0.004, 0.007, 0.009, 0.007, 0.013,\n",
       "       0.011, 0.012, 0.012, 0.003, 0.014, 0.017, 0.012, 0.015, 0.003,\n",
       "       0.014, 0.01 , 0.013, 0.008, 0.001, 0.006, 0.012, 0.004, 0.008,\n",
       "       0.003, 0.014, 0.01 , 0.011, 0.013, 0.006, 0.009, 0.007, 0.014,\n",
       "       0.013, 0.002, 0.013, 0.015, 0.013, 0.013, 0.001, 0.002, 0.002,\n",
       "       0.   , 0.003, 0.005, 0.01 , 0.007, 0.016, 0.011, 0.001, 0.009,\n",
       "       0.007, 0.01 , 0.011, 0.003, 0.009, 0.016, 0.006, 0.01 , 0.001,\n",
       "       0.004, 0.002, 0.004, 0.005, 0.004, 0.017, 0.012, 0.013, 0.01 ,\n",
       "       0.004, 0.016, 0.014, 0.01 , 0.011, 0.001, 0.013, 0.012, 0.006,\n",
       "       0.01 , 0.004, 0.002, 0.007, 0.007, 0.008, 0.002, 0.011, 0.008,\n",
       "       0.022, 0.014, 0.005, 0.017, 0.015, 0.007, 0.013, 0.005, 0.01 ,\n",
       "       0.011, 0.01 , 0.011])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_win_probs_Ia(X_possibilities, thetas): \n",
    "    Ias = np.zeros((len(thetas), len(X_possibilities)))\n",
    "    for i, theta in enumerate(thetas): \n",
    "        Ias[i,:] = calculate_Ia(X_possibilities, theta)\n",
    "    return np.mean(Ias, axis=0)\n",
    "compute_win_probs_Ia(X_possibilities([2,3,4,5]), np.random.multivariate_normal(np.zeros(11), np.eye(11, dtype=int), size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_bandit_experiment(n_arms, levels, n_trials, sigma, true_success_probs, strategy=\"rpm\"): \n",
    "\n",
    "    X_possibilities = X_possibilities([levels])\n",
    "    rewards = calculate_reward(X_possibilities, true_success_probs)\n",
    "    optimal_reward = calculate_best_reward(X_possibilities, true_success_probs)\n",
    "    \n",
    "    thetas = \n",
    "    regrets = []\n",
    "    cumulative_regret = 0\n",
    "    \n",
    "    n_observations = 1000 \n",
    "    obs = [[] for _ in range(n_arms)]\n",
    "    for t in range(1, n_trials + 1):\n",
    "        win_probs = compute_win_probs_Ia(X_possibilities, thetas) # w_at for each arm\n",
    "        if strategy==\"rpm\":\n",
    "            chosen_arms = np.random.choice(range(n_arms), p=win_probs, size=n_observations)\n",
    "        if strategy==\"equal allocation\":\n",
    "            chosen_arms = np.random.choice(range(n_arms), p=[1/n_arms]*n_arms, size=n_observations)\n",
    "    \n",
    "        n_t = np.zeros(n_arms)  # Nombre de tirages pour chaque bras à cette étape\n",
    "        np.add.at(n_t, chosen_arms-1, 1)\n",
    "\n",
    "        # Actualiser Y_t et calculer le regret \n",
    "        regret_t = 0\n",
    "        for a in range(n_arms):\n",
    "            obs[a].append = rewards[a] + np.random.normal(0, sigma**2, size=n_t[a]) # Simule des observations selon la probabilité réelle de succès de ce bras\n",
    "\n",
    "            regret_t +=  n_t[a]*(optimal_reward - rewards[a]) \n",
    "        regrets.append(regret_t)\n",
    "        cumulative_regret += regret_t\n",
    "\n",
    "        thetas = generates_theta(X_t, y_t, sigma)\n",
    "        \n",
    "        if max(win_probs)>0.95: \n",
    "            break\n",
    "\n",
    "        \n",
    "    return regrets, cumulative_regret\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
